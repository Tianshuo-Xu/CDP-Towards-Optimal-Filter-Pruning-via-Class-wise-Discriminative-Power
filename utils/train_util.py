import time

import torch
import torchvision.transforms as transforms
import torchvision
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from .scheduler import linear_warmup_scheduler
device = 'cuda' if torch.cuda.is_available() else 'cpu'

class LabelSmoothCELoss(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing=smoothing
        print('label smoothing:', self.smoothing)
        
    def forward(self, pred, label):
        pred = F.softmax(pred, dim=1)
        one_hot_label = F.one_hot(label, pred.size(1)).float()
        smoothed_one_hot_label = (
            1.0 - self.smoothing) * one_hot_label + self.smoothing / pred.size(1)
        loss = (-torch.log(pred)) * smoothed_one_hot_label
        loss = loss.sum(axis=1, keepdim=False)
        loss = loss.mean()

        return loss

def train(net, epochs, lr, train_loader, test_loader, save_info='./', save_acc=80.0, start_epoch=0, device='cuda', log_every_n=50, **kargs):
    """
    Training a network
    :param net: Network for training
    :param epochs: Number of epochs in total.
    :param batch_size: Batch size for training.
    """
    #print('==> Preparing data..')
    criterion = LabelSmoothCELoss(kargs["label_smoothing"])

    criterion = nn.CrossEntropyLoss()

    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4, nesterov=True)
    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs*0.5), int(epochs*0.75)], gamma=0.1)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=0)

    warmup_scheduler = linear_warmup_scheduler(optimizer, kargs["warmup_step"], kargs["warm_lr"], lr)
  
    best_acc = 0  # best test accuracy
    for epoch in range(start_epoch, epochs):
        """
        Start the training code.
        """
        print('\nEpoch: %d' % epoch, '/ %d;' % epochs, 'learning_rate:', optimizer.state_dict()['param_groups'][0]['lr'])
        net.train()
        train_loss = 0
        correct = 0
        total = 0
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            # 如果 warmup scheduler=None 或者不在 warmup 范围里， if_warmup=False
            if_warmup=False if warmup_scheduler==None else warmup_scheduler.if_in_warmup()

            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = net(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            
            optimizer.step()

            if if_warmup:
                warmup_scheduler.step()

            # clear masked weights to zero
            '''for n, m in net.named_modules():
                if isinstance(m, nn.Conv2d):
                    mask = m.mask
                    m.weight.data *= mask
                    m.weight.grad.data *= mask'''
                    
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        print("Train Loss=%.8f, Train acc=%.8f"
              % (train_loss / (batch_idx + 1), (correct / total)))
        
        if not warmup_scheduler or not warmup_scheduler.if_in_warmup():
            scheduler.step()

        """
        Start the testing code.
        """
        net.eval()
        test_loss = 0
        correct = 0
        total = 0
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(test_loader):
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = net(inputs)
                loss = criterion(outputs, targets)

                test_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        num_val_steps = len(test_loader)
        val_acc = correct / total
        if(kargs.__contains__("ablation") and kargs["ablation"][0]<val_acc):
            kargs["ablation"][0]=val_acc

        if val_acc*100 > save_acc:
            save_acc = val_acc*100
            info = save_info+'_'+str(save_acc)[0:5]+'.pth'
            print(info)
            torch.save(net.state_dict(), info)
        print("Test Loss=%.8f, Test acc=%.8f" % (test_loss / (num_val_steps), val_acc))

def test(net, testloader):

    criterion = nn.CrossEntropyLoss()

    net.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(testloader):
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, targets)

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    num_val_steps = len(testloader)
    val_acc = correct / total
    print("Test Loss=%.4f, Test accuracy=%.4f" % (test_loss / (num_val_steps), val_acc))
    loss = test_loss / num_val_steps
    return loss, val_acc

def load_model_pytorch(model, load_model, model_name):
    #print("=> loading checkpoint '{}'".format(load_model))
    checkpoint = torch.load(load_model)

    if 'state_dict' in checkpoint.keys():
        load_from = checkpoint['state_dict']
    else:
        load_from = checkpoint

    # match_dictionaries, useful if loading model without gate:
    if 'module.' in list(model.state_dict().keys())[0]:
        if 'module.' not in list(load_from.keys())[0]:
            from collections import OrderedDict

            load_from = OrderedDict([("module.{}".format(k), v) for k, v in load_from.items()])

    if 'module.' not in list(model.state_dict().keys())[0]:
        if 'module.' in list(load_from.keys())[0]:
            from collections import OrderedDict

            load_from = OrderedDict([(k.replace("module.", ""), v) for k, v in load_from.items()])

    # just for vgg
    if model_name == "vgg":
        from collections import OrderedDict

        load_from = OrderedDict([(k.replace("features.", "features"), v) for k, v in load_from.items()])
        load_from = OrderedDict([(k.replace("classifier.", "classifier"), v) for k, v in load_from.items()])

    if 1:
        for ind, (key, item) in enumerate(model.state_dict().items()):
            if ind > 10:
                continue
            #print(key, model.state_dict()[key].shape)

        #print("*********")

        for ind, (key, item) in enumerate(load_from.items()):
            if ind > 10:
                continue
            #print(key, load_from[key].shape)

    for key, item in model.state_dict().items():
        # if we add gate that is not in the saved file
        if key not in load_from:
            load_from[key] = item
        # if load pretrined model
        if load_from[key].shape != item.shape:
            load_from[key] = item

    model.load_state_dict(load_from, False)


    epoch_from = -1
